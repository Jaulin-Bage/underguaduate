{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb618b11-d1a1-4206-bfcb-588ac3a6a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn,einsum\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92fc81aa-e1b6-456c-aedf-87240bf40cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "#data_path = \"./jpegs_256/\"    # define UCF-101 RGB data path\n",
    "data_path = \"/home/hanpeiheng/dataset/yawdd_temp/\"\n",
    "action_name_path = './data.pkl' #全部动作的名字标签\n",
    "save_model_path = \"../vivit_ckpt/\"\n",
    "#fnames = os.listdir(data_path)\n",
    "#fnames\n",
    "\n",
    "# EncoderCNN architecture\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 256, 128\n",
    "CNN_embed_dim = 256      # latent dim extracted by 2D CNN\n",
    "img_x, img_y = 80,80  # resize video 2d frame size\n",
    "img_size = 80\n",
    "ptc_size=20\n",
    "dropout_p = 0.3          # dropout probability 丢失概率设置为0.0，这意味着不会丢弃任何神经元。这表明该模型可能不太复杂，过拟合可能不是一个主要问题。\n",
    "\n",
    "# DecoderRNN architecture\n",
    "RNN_hidden_layers = 3 # 三个隐藏层\n",
    "RNN_hidden_nodes = 128 # 每个隐藏层512个节点\n",
    "RNN_FC_dim = 128 # 一个全连接层，其维度为256\n",
    "\n",
    "# training parameters\n",
    "k = 3             # number of target category 目标类别的数量为101个\n",
    "epochs = 100        # training epochs 训练轮数\n",
    "batch_size = 31     # 每批次训练的样本数量为30\n",
    "learning_rate = 1e-4 # 学习率为0.0001  学习率参数很重要，自己搜搜看吧\n",
    "log_interval = 10   # interval for displaying training info 训练过程中打印训练信息的间隔为10\n",
    "lam=1e-4\n",
    "weight_decay_global=1e-6\n",
    "step_size=10\n",
    "gamma=0.9\n",
    "\n",
    "# Select which frame to begin & end in videos\n",
    "begin_frame, end_frame, skip_frame = 30, 90, 2 #begin_frame表示从第1帧开始，end_frame表示结束帧数为29，skip_frame表示每隔1帧进行采样，即不对连续帧进行处理。这些参数通常用于视频分类或动作识别任务中\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2babe42c-7bc1-4771-8829-d1b6b8fd0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入参数：包括日志输出间隔(log_interval)、模型(model)，设备(device)、训练数据加载器(train_loader)、优化器(optimizer)和当前epoch\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    en = model[0]\n",
    "    de = model[1]\n",
    "    en.train()\n",
    "    de.train()\n",
    "\n",
    "    losses = [] # 损失率\n",
    "    scores = [] # 准确率\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        # 对于每个批次(batch)，它将数据(X, y)分发到给定的设备上(device)\n",
    "        X, y = X.to(device), y.to(device).view(-1, )\n",
    "\n",
    "        N_count += X.size(0)\n",
    "        \n",
    "        # 模型的前向传递通过使用卷积神经网络编码器(cnn_encoder)对输入数据进行特征提取\n",
    "        # 并使用循环神经网络解码器(rnn_decoder)对特征进行分类输出(output)\n",
    "        optimizer.zero_grad()\n",
    "        output = de(en(X))   # output has dim = (batch, number of classes)\n",
    "        \n",
    "        # 计算输出(output)与标签(y)之间的交叉熵损失(loss)\n",
    "        re_loss = 0\n",
    "        for name, param in model[0].named_parameters():\n",
    "            if param.requires_grad:\n",
    "                re_loss += torch.sum(torch.abs(param))\n",
    "        for name, param in model[1].named_parameters():\n",
    "            if param.requires_grad:\n",
    "                re_loss += torch.sum(torch.abs(param))\n",
    "        loss = F.cross_entropy(output, y) + re_loss * lam\n",
    "        losses.append(loss.item()) # 将该损失值添加到损失列表(losses)\n",
    "\n",
    "        # to compute accuracy\n",
    "        # 使用 torch.max() 函数检索输出张量(output)中最大值的索引作为预测标签(y_pred)\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        # 将预测标签与实际标签(y)进行比较。使用 accuracy_score() 函数计算预测准确率(step_score)\n",
    "        # 并将其添加到准确度列表(scores)中\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "        \n",
    "        # 反向传播将梯度更新到各个网络层\n",
    "        # 并执行优化步骤(optimizer.step())来更新模型参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        # 如果达到了日志输出间隔(log_interval)，则打印训练信息，包括当前epoch、批次进度、损失和准确率\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch , N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "\n",
    "    return sum(losses)/len(losses), sum(scores)/len(scores)\n",
    "\n",
    "# 输入参数：接受一个模型、设备、优化器和测试数据集作为参数\n",
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    # 在没有梯度计算的情况下进行测试\n",
    "    en = model[0]\n",
    "    de = model[1]\n",
    "    en.eval() # 将模型设置为评估（testing）模式\n",
    "    de.eval() # 将模型设置为评估（testing）模式\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        # 循环遍历测试数据集中的所有批次\n",
    "        # 将每个批次的输入数据X和标签y分配到指定的设备上\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device).view(-1, )\n",
    "            # 使用模型对输入数据进行前向传递并计算输出结果\n",
    "            output = de(en(X))\n",
    "            # 使用交叉熵损失函数计算输出结果和实际标签的损失\n",
    "            re_loss = 0\n",
    "            for name, param in model[0].named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    re_loss += torch.sum(torch.abs(param))\n",
    "            \n",
    "            for name, param in model[1].named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    re_loss += torch.sum(torch.abs(param))\n",
    "            \n",
    "                    \n",
    "            loss = F.cross_entropy(output, y, reduction='sum') + re_loss * lam\n",
    "            test_loss += loss.item()                 # sum up batch loss 将所有批次的损失相加以计算平均损失\n",
    "            # 将输出结果转换为预测标签\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            # 将所有批次的实际标签和预测标签收集到all_y和all_y_pred列表中\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    # 计算测试准确率\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('Test set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    # 将当前模型的状态字典（state_dict）保存到指定路径\n",
    "    torch.save(en.state_dict(), os.path.join(save_model_path, 'en_epoch{}.pth'.format(epoch)))  # save spatial_encoder\n",
    "    torch.save(de.state_dict(), os.path.join(save_model_path, 'de_epoch{}.pth'.format(epoch)))  # save spatial_encoder\n",
    "    # torch.save(rnn_decoder.state_dict(), os.path.join(save_model_path, 'rnn_decoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch))\n",
    "    \n",
    "    # 返回测试损失和测试准确率\n",
    "    return test_loss, test_score\n",
    "\n",
    "\n",
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "# Data loading parameters\n",
    "# 参数介绍 # ↓\n",
    "# batch_size: 批处理大小，指定每个批次(batch)中包含的样本数量\n",
    "# shuffle: 是否对数据进行洗牌，即随机打乱顺序。如果设置为 True，则每个 epoch(整个数据集被遍历一次)都会重新打乱数据\n",
    "# num_workers: 使用多少个子进程来加载数据。默认值是 0，表示在主进程中加载数据。如果设置为大于 0 的数值，则使用多个子进程同时读取数据，可以加快数据加载速度\n",
    "# pin_memory: 是否将数据放置在 CUDA 主机内存上。如果使用 GPU 计算，这个参数可以提高数据加载速度\n",
    "# if use_cuda else {} 根据变量 use_cuda 是否为 True 来决定是否将字典设为空字典 {}。这里的意思是，\n",
    "# 如果 use_cuda 为 True，那么就使用前面定义的 batch_size、shuffle、num_workers 和 pin_memory 这些参数；否则，就使用空字典\n",
    "# 这个判断是由于在使用 GPU 时，需要将参数数据字典放在显存上，而在 CPU 上使用时则不需要\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "468fca56-1934-4c88-97c4-f4c9542f79f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanpeiheng/anaconda3/envs/test/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hanpeiheng/anaconda3/envs/test/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load UCF101 actions names\n",
    "# 加载101个视频动作名字\n",
    "with open(action_name_path, 'rb') as f:\n",
    "    action_names = pickle.load(f)\n",
    "    \n",
    "# convert labels -> category\n",
    "# LabelEncoder 是 scikit-learn 库中的一个工具类，可以将分类变量编码为数值变量\n",
    "# 定义了一个 LabelEncoder 的实例对象 le，然后使用 fit() 方法来拟合数据（即学习标签与类别之间的对应关系）。fit() 方法的参数为 action_names，即标签名列表\n",
    "# 例如，如果 action_names 中包含三个不同的标签名：'A'、'B'、'C'，那么执行 fit() 后，le.classes_ 属性会被设置为 ['A', 'B', 'C']，\n",
    "le = LabelEncoder()\n",
    "le.fit(action_names)\n",
    "\n",
    "# show how many classes there are\n",
    "list(le.classes_)\n",
    "\n",
    "# convert category -> 1-hot\n",
    "# OneHotEncoder() 是 scikit-learn 库中的一个工具类，可以将分类变量编码为 one-hot 向量\n",
    "# 使用前面定义的 LabelEncoder 对标签(label)进行编码，得到类别序号(category index)。\n",
    "# 这一步使用了 le.transform(action_names) 方法将 action_names 列表中的每个标签映射为对应的类别序号\n",
    "# 接着上面的例子就是le.transform(['A', 'B', 'C']) 会返回一个数组 [0, 1, 2]，表示 'A' 被编码为 0，'B' 编码为 1，'C' 编码为 2。\n",
    "# 并通过 reshape(-1, 1) 将结果转换为列向量如下：\n",
    "#array([[0],\n",
    "#       [1],\n",
    "#      [2]])\n",
    "action_category = le.transform(action_names).reshape(-1, 1)\n",
    "# 创建一个 OneHotEncoder 的实例对象 enc\n",
    "# 并使用 fit() 方法拟合数据（即学习各类别之间的关系）\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(action_category)\n",
    "\n",
    "# # example\n",
    "# y = ['HorseRace', 'YoYo', 'WalkingWithDog']\n",
    "# y_onehot = labels2onehot(enc, le, y)\n",
    "# y2 = onehot2labels(le, y_onehot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e010c210-3cd2-4279-88b3-03bd32d286bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "actions = []\n",
    "fnames = os.listdir(data_path)\n",
    "\n",
    "all_names = []\n",
    "for f in fnames:\n",
    "    loc1 = f.find('s-')\n",
    "    if loc1==-1:\n",
    "        loc1=f.find('d-')\n",
    "        \n",
    "        if loc1 == -1:\n",
    "            loc1=f.find('e-')\n",
    "            \n",
    "    actions.append(f[(loc1 + 2): ])\n",
    "\n",
    "    all_names.append(f)\n",
    "\n",
    "all_X_list = all_names\n",
    "all_y_list = labels2cat(le, actions)    \n",
    "\n",
    "train_list, test_list, train_label, test_label = train_test_split(all_X_list, all_y_list, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "selected_frames = np.arange(begin_frame, end_frame, skip_frame).tolist()\n",
    "\n",
    "# 反斜线 \\ 是 Python 中的行连接符，用于将一行代码分成多行来提高代码的可读性和易维护性\n",
    "train_set, valid_set = Dataset_CRNN(data_path, train_list, train_label, selected_frames, transform=transform), \\\n",
    "                       Dataset_CRNN(data_path, test_list, test_label, selected_frames, transform=transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_set, **params)\n",
    "valid_loader = data.DataLoader(valid_set, **params)\n",
    "\n",
    "# Create model\n",
    "# EncoderCNN和DecoderRNN都是functions.py的自定义类\n",
    "en = ViViT_en(image_size=img_size, patch_size=ptc_size, num_frames=30,dropout=dropout_p).to(device)\n",
    "de = LSTM_de(CNN_embed_dim=32, h_RNN_layers=2, h_RNN=256, h_FC_dim=128, drop_p=0.3, num_classes=3).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    en = nn.DataParallel(en) #分配到可用的GPU上\n",
    "    de = nn.DataParallel(de) #分配到可用的GPU上\n",
    "# 作为一个整体被优化器所优化\n",
    "params = list(en.parameters())+list(de.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate,weight_decay=weight_decay_global)\n",
    "StepLR = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ac433a8-d5d0-42e5-b9b6-5b99855e49a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [400/626 (62%)]\tLoss: 5.279427, Accu: 47.50%\n",
      "Test set (157 samples): Average loss: 1.1929, Accuracy: 42.04%\n",
      "\n",
      "Epoch 0 model saved!\n",
      "Train Epoch: 1 [400/626 (62%)]\tLoss: 5.163776, Accu: 32.50%\n",
      "Test set (157 samples): Average loss: 1.1845, Accuracy: 42.04%\n",
      "\n",
      "Epoch 1 model saved!\n",
      "Train Epoch: 2 [400/626 (62%)]\tLoss: 5.031494, Accu: 35.00%\n",
      "Test set (157 samples): Average loss: 1.1799, Accuracy: 42.04%\n",
      "\n",
      "Epoch 2 model saved!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# start training\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# train, test model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     train_losses, train_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43men\u001b[49m\u001b[43m,\u001b[49m\u001b[43mde\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     epoch_test_loss, epoch_test_score \u001b[38;5;241m=\u001b[39m validation([en,de], device, optimizer, valid_loader)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# save results\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# train_scores_avg=sum(train_scores)/len(train_scores)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(log_interval, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m scores \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# 准确率\u001b[39;00m\n\u001b[1;32m     11\u001b[0m N_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m   \u001b[38;5;66;03m# counting total trained sample in one epoch\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# distribute data to device\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# 对于每个批次(batch)，它将数据(X, y)分发到给定的设备上(device)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, )\n\u001b[1;32m     17\u001b[0m     N_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1272\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1272\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1274\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []\n",
    "\n",
    "# start training\n",
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    train_losses, train_scores = train(log_interval, [en,de], device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score = validation([en,de], device, optimizer, valid_loader)\n",
    "\n",
    "    # save results\n",
    "    # train_scores_avg=sum(train_scores)/len(train_scores)\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_train_scores.append(train_scores)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "    epoch_test_scores.append(epoch_test_score)\n",
    "\n",
    "    # save all train test results\n",
    "    A = np.array(epoch_train_losses)\n",
    "    B = np.array(epoch_train_scores)\n",
    "    C = np.array(epoch_test_losses)\n",
    "    D = np.array(epoch_test_scores)\n",
    "    np.save('./vivit_epoch_training_losses.npy', A)\n",
    "    np.save('./vivit_epoch_training_scores.npy', B)\n",
    "    np.save('./vivit_epoch_test_loss.npy', C)\n",
    "    np.save('./vivit_epoch_test_score.npy', D)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(10, 4)) #创建一个10x4英寸的新窗口\n",
    "plt.subplot(121) # 参数121表示将整个图形窗口分成1行2列，在第1个位置上添加子图，设为当前绘图区域\n",
    "#  绘制折线图，在当前子图中绘制以训练时期为横坐标、损失值为纵坐标的折线图\n",
    "# 参数np.arange(1, epochs + 1)用于生成一个1到epochs的整数序列，表示训练时期的编号\n",
    "# A[:, -1]表示将数组A的最后一列作为y轴的数据，即每个训练时期的最后一个batch的损失值\n",
    "plt.plot(np.arange(1, epochs + 1), A)  # train loss (on epoch end)\n",
    "plt.plot(np.arange(1, epochs + 1), C)         #  test loss (on epoch end)\n",
    "plt.title(\"model loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "# 2nd figure \n",
    "#代码解释和上面的同理\n",
    "plt.subplot(122)\n",
    "plt.plot(np.arange(1, epochs + 1), B)  # train accuracy (on epoch end)\n",
    "plt.plot(np.arange(1, epochs + 1), D)         #  test accuracy (on epoch end)\n",
    "plt.title(\"training scores\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "title = \"./fig_yawdd_vivit.png\"\n",
    "plt.savefig(title, dpi=600)\n",
    "# plt.close(fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d1b76-54b8-4c63-bfc3-aa949968b751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
